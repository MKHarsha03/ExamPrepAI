{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import getpass\n",
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key=os.getenv(\".env\")\n",
    "#os.environ['GROQ_API_KEY']=getpass.getpass(api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path=\"./test_textbook/Compilers Principles, Techniques, & Tools 2nd Ed.pdf\"\n",
    "loader=PyPDFLoader(file_path)\n",
    "\n",
    "docs=loader.load()\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings=OllamaEmbeddings(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3201"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,chunk_overlap=200,add_start_index=True\n",
    ")\n",
    "all_splits=text_splitter.split_documents(docs)\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store=InMemoryVectorStore(embeddings)\n",
    "ids=vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='T o see ho w the GOTO /'s are computed/, consider GOTO /#28 I\n",
      "/3/6\n",
      "/;C /#29/. In the original\n",
      "set of LR/#28/1/#29 items/, GOTO /#28 I\n",
      "/3\n",
      "/; C /#29/= I\n",
      "/8\n",
      "/, and I\n",
      "/8\n",
      "is no w part of I\n",
      "/8/9\n",
      "/,s o w e mak e\n",
      "GOTO /#28 I\n",
      "/3/6\n",
      "/;C /#29 be I\n",
      "/8/9\n",
      "/. W e could ha v e arriv ed at the same conclusion if w e\n",
      "considered I\n",
      "/6\n",
      "/, the other part of I\n",
      "/3/6\n",
      "/. That is/, GOTO /#28 I\n",
      "/6\n",
      "/;C /#29 /= I\n",
      "/9\n",
      "/, and I\n",
      "/9\n",
      "is\n",
      "no w part of I\n",
      "/8/9\n",
      "/. F or another example/, consider GOTO /#28 I\n",
      "/2\n",
      "/;c /#29/, an en try that is\n",
      "exercised after the shift action of I\n",
      "/2\n",
      "on input c /. In the original sets of LR/#28/1/#29\n",
      "items/, GOTO /#28 I\n",
      "/2\n",
      "/;c /#29/= I\n",
      "/6\n",
      "/. Since I\n",
      "/6\n",
      "is no w part of I\n",
      "/3/6\n",
      "/, GOTO /#28 I\n",
      "/2\n",
      "/;c /#29 b ecomes I\n",
      "/3/6\n",
      "/.\n",
      "Th us/, the en try in Fig/. /4/./4/3 for state /2 and input c is made s/3/6/, meaning shift\n",
      "and push state /3/6 on to the stac k/. /2\n",
      "When presen ted with a string from the language c\n",
      "/#03\n",
      "dc\n",
      "/#03\n",
      "d /, b oth the LR parser' metadata={'source': './test_textbook/Compilers Principles, Techniques, & Tools 2nd Ed.pdf', 'page': 273, 'start_index': 833}\n"
     ]
    }
   ],
   "source": [
    "query=\"Explain lexical analyzer\"\n",
    "results=vector_store.similarity_search(query)\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Lexical Analysis: Understanding Lexical Analyzer**\n",
      "\n",
      "**Contextual Analysis**\n",
      "\n",
      "The given context is related to the construction of LR parser tables and the computation of GOTOs. The lexical analyzer is a crucial component of this process, as it breaks the input source code into a stream of tokens, which are then used to build the parser.\n",
      "\n",
      "**Function of Lexical Analyzer**\n",
      "\n",
      "A lexical analyzer, also known as a lexer or scanner, performs the following functions:\n",
      "\n",
      "• **Tokenization**: breaks the input source code into a stream of tokens, such as keywords, identifiers, literals, symbols, and other special characters.\n",
      "\n",
      "• **Pattern Matching**: matches the tokens against a set of predefined patterns or regular expressions to identify the types of tokens.\n",
      "\n",
      "• **Error Handling**: checks for any errors in the input source code, such as syntax errors or invalid characters, and reports them to the parser.\n",
      "\n",
      "**Example: Tokenization**\n",
      "\n",
      "In the given context, the input string `T o see ho w the GOTO /'s are computed/, consider GOTO /#28 I /3/6 /;C /#29/.` would be tokenized into the following tokens:\n",
      "\n",
      "• `T` (keyword)\n",
      "• `o` (identifier)\n",
      "• `see` (keyword)\n",
      "• `ho` (identifier)\n",
      "• `w` (keyword)\n",
      "• `the` (keyword)\n",
      "• `GOTO` (keyword)\n",
      "• `/#28` (integer literal)\n",
      "• `I` (identifier)\n",
      "• `/3/6` (integer literal)\n",
      "• `;C` (symbol)\n",
      "• `/#29` (integer literal)\n",
      "\n",
      "The lexical analyzer would then pass these tokens to the parser for further analysis.\n",
      "\n",
      "**Note:** The lexical analyzer is a critical component of the compiler, as it sets the stage for the rest of the compilation process. Its output is used to build the parser, which in turn analyzes the source code to produce an abstract syntax tree (AST).\n"
     ]
    }
   ],
   "source": [
    "client = Groq(api_key=api_key)\n",
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {'role':'system','content':'''Answer the questions from the prompt and the context given by the user. If the answer is not\n",
    "         found, reply \"Cannot provide answer\", don't give any additional explanation about the question.Give the answer with proper headings,subheadings and bullet points if it is a long answer.\n",
    "         You have to help the user understand the answer to the question and format it for notes making.Do not give answers from outside provided context'''},\n",
    "         {'role':'user','content':f\"Context:{results[0]},Question:{query}\"}\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
