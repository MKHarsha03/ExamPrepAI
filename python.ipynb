{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key=os.getenv(\".env\")\n",
    "#os.environ['GROQ_API_KEY']=getpass.getpass(api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path=r\"C:\\Users\\khmam\\Desktop\\ExamPrepAI\\test_textbook\\Compilers Principles, Techniques, & Tools 2nd Ed.pdf\"\n",
    "loader=PyPDFLoader(file_path)\n",
    "\n",
    "docs=loader.load()\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings=OllamaEmbeddings(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3201"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,chunk_overlap=200,add_start_index=True\n",
    ")\n",
    "all_splits=text_splitter.split_documents(docs)\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store=InMemoryVectorStore(embeddings)\n",
    "ids=vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'238, 241-242, 260, 267, 283-284, 295\\nInput variables, 686\\nInsert, 98, 114, 196, 231, 233, 350, 362, 370, 410,\\n462, 642, 647, 653, 666, 730, 760, 766, 898,\\n911\\ninserting, 87, 558, 709, 874, 904\\nInsertion, 284, 764, 767\\nInstance, 26-27, 30, 35, 52, 58, 74, 77, 82, 85, 92,\\n110-111, 113, 115, 117, 119, 122, 124-125,\\n129, 137, 142, 144-146, 148-149, 170, 175,\\n180, 184-187, 200, 209, 221, 224, 269, 284,\\n304, 307, 310, 326, 328, 340, 342, 359, 361,\\n366, 374, 379-380, 386, 393, 404, 444, 449,\\n499, 531, 537-539, 583-584, 592, 594, 598,\\n601, 603, 608-609, 643, 650, 666, 683, 685,\\n688, 694, 724, 736, 753, 758, 792, 809, 811,\\n816, 819, 830, 833, 839, 850, 855-856,\\n858-859, 864, 867, 869-870, 888\\nInstances, 56, 122, 124, 310, 327, 353, 393, 583-584,\\n614, 645, 747, 770, 779, 804-805, 816-817,\\n832, 835, 840, 852, 854-856, 858, 870, 885,\\n888-889\\ninstruction set, 19-22, 507-509, 512, 560, 565-566,\\n579, 710, 714, 719, 766\\nInstruction-level parallelism, 19, 707-708, 710,'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"Explain in detail lexical analyser.\"\n",
    "results=vector_store.similarity_search(query)\n",
    "results[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Lexical Analyzer**\n",
      "\n",
      "The Lexical Analyzer, also known as the Scanner or Tokenizer, is the first phase of the compiler. Its primary function is to break the source code into meaningful units called tokens, which are then fed into the parser. These tokens are the building blocks of the programming language and are used to construct the parse tree.\n",
      "\n",
      "**Functionality**\n",
      "\n",
      "The Lexical Analyzer's primary function is to:\n",
      "\n",
      "1. **Tokenization**: Break the source code into tokens, which are the basic units of the programming language. Tokens can be keywords, identifiers, literals, symbols, and whitespace characters.\n",
      "2. **Lexical Analysis**: Analyze the tokens and determine their meaning. This includes identifying keywords, operators, literals, and identifiers.\n",
      "3. **Error Detection**: Detect and report any lexical errors, such as syntax errors, unmatched parentheses, or undefined identifiers.\n",
      "\n",
      "**Components**\n",
      "\n",
      "The Lexical Analyzer typically consists of the following components:\n",
      "\n",
      "1. ** Finite State Machine (FSM)**: A FSM is used to recognize the tokens in the source code. The FSM is defined by a set of states and transitions, which are triggered by the input characters.\n",
      "2. **Regular Expressions**: Regular expressions are used to define patterns for the tokens. The Lexical Analyzer uses these patterns to recognize the tokens and determine their meaning.\n",
      "3. **Error Handler**: The Error Handler is responsible for detecting and reporting any lexical errors.\n",
      "\n",
      "**Example**\n",
      "\n",
      "Let's consider an example of a simple arithmetic expression: `a + 5`. The Lexical Analyzer would break this into the following tokens:\n",
      "\n",
      "* `a` (identifier)\n",
      "* `+` (operator)\n",
      "* `5` (literal)\n",
      "\n",
      "The Lexical Analyzer would then analyze these tokens and determine their meaning. It would recognize `a` as an identifier, `+` as an operator, and `5` as a literal.\n",
      "\n",
      "**Implementation**\n",
      "\n",
      "The Lexical Analyzer can be implemented using various techniques, including:\n",
      "\n",
      "1. **Hand-coded implementation**: The Lexical Analyzer can be implemented by writing a series of if-else statements or case statements to handle each type of token.\n",
      "2. **Finite State Machine (FSM) implementation**: The Lexical Analyzer can be implemented using a FSM, which is defined by a set of states and transitions. This approach is more efficient and scalable.\n",
      "3. **Regular Expression (regex) implementation**: The Lexical Analyzer can be implemented using regex patterns to recognize the tokens. This approach is flexible and easy to maintain.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "In conclusion, the Lexical Analyzer is a crucial phase of the compiler that breaks the source code into meaningful units called tokens. It analyzes the tokens and determines their meaning, and detects any lexical errors. The Lexical Analyzer can be implemented using various techniques, including hand-coded implementation, FSM implementation, and regex implementation.\n"
     ]
    }
   ],
   "source": [
    "client = Groq(api_key=api_key)\n",
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {'role':'system','content':'''Answer the questions from the prompt and the context given by the user. If the answer is not\n",
    "         found, reply \"Cannot provide answer\", don't give any additional explanation about the question.Give the answer with proper headings,subheadings and bullet points if it is a long answer.\n",
    "         You have to help the user understand the answer to the question and format it for notes making.Do not give answers from outside provided context'''},\n",
    "         {'role':'user','content':f\"Context:{results[:5]},Question:{query}\"}\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
